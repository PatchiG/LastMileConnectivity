{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session dda01b26-dea3-477d-8e94-24b339ebb7e9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session dda01b26-dea3-477d-8e94-24b339ebb7e9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session dda01b26-dea3-477d-8e94-24b339ebb7e9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session dda01b26-dea3-477d-8e94-24b339ebb7e9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\nfrom pyspark.sql.functions import col",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Initialize Spark session\nspark = SparkSession.builder.appName(\"SchemaValidation\").getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Define expected schema for capital_rideshare_combined.csv\nexpected_schema_rideshare = StructType([\n    StructField(\"ride_id\", StringType(), True),\n    StructField(\"rideable_type\", StringType(), True),\n    StructField(\"started_at\", TimestampType(), True),\n    StructField(\"ended_at\", TimestampType(), True),\n    StructField(\"start_station_name\", StringType(), True),\n    StructField(\"start_station_id\", IntegerType(), True),\n    StructField(\"end_station_name\", StringType(), True),\n    StructField(\"end_station_id\", IntegerType(), True),\n    StructField(\"start_lat\", DoubleType(), True),\n    StructField(\"start_lng\", DoubleType(), True),\n    StructField(\"end_lat\", DoubleType(), True),\n    StructField(\"end_lng\", DoubleType(), True),\n    StructField(\"member_casual\", StringType(), True),\n    StructField(\"year\", IntegerType(), True),\n    StructField(\"month\", IntegerType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Load data from S3\ns3_path_rideshare = \"s3://capitalbikesharedata/bikeshare/capital_rideshare_combined.csv\"\ndata_rideshare = spark.read.csv(s3_path_rideshare, header=True, inferSchema=True)\n\n# Log the actual schema\nprint(\"Actual schema for capital_rideshare_combined.csv:\")\nprint(data_rideshare.schema)\n\n# Function to validate schema field by field\ndef validate_schema(expected_schema, actual_schema):\n    failed_fields = []\n    \n    for expected, actual in zip(expected_schema, actual_schema):\n        if expected.name != actual.name:\n            failed_fields.append(f\"Field name mismatch: Expected '{expected.name}', Found '{actual.name}'\")\n        elif type(expected.dataType) != type(actual.dataType):\n            failed_fields.append(f\"Field type mismatch: Field '{expected.name}' - Expected '{expected.dataType}', Found '{actual.dataType}'\")\n    \n    if failed_fields:\n        print(\"Schema validation failed for capital_rideshare_combined.csv\")\n        for error in failed_fields:\n            print(error)\n    else:\n        print(\"Schema validation successful for capital_rideshare_combined.csv\")\n\n# Validate schema\nvalidate_schema(expected_schema_rideshare, data_rideshare.schema)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "Actual schema for capital_rideshare_combined.csv:\nStructType([StructField('ride_id', StringType(), True), StructField('rideable_type', StringType(), True), StructField('started_at', StringType(), True), StructField('ended_at', StringType(), True), StructField('start_station_name', StringType(), True), StructField('start_station_id', IntegerType(), True), StructField('end_station_name', StringType(), True), StructField('end_station_id', IntegerType(), True), StructField('start_lat', DoubleType(), True), StructField('start_lng', DoubleType(), True), StructField('end_lat', DoubleType(), True), StructField('end_lng', DoubleType(), True), StructField('member_casual', StringType(), True), StructField('year', IntegerType(), True), StructField('month', IntegerType(), True)])\nSchema validation failed for capital_rideshare_combined.csv\nField type mismatch: Field 'started_at' - Expected 'TimestampType()', Found 'StringType()'\nField type mismatch: Field 'ended_at' - Expected 'TimestampType()', Found 'StringType()'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Define expected schema for placemarks_data.csv\nexpected_schema_placemarks = StructType([\n    StructField(\"id\", StringType(), True),\n    StructField(\"networkName\", StringType(), True),\n    StructField(\"networkId\", IntegerType(), True),\n    StructField(\"color\", StringType(), True),\n    StructField(\"textColor\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"location\", StringType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Load data from S3\ns3_path_placemarks = \"s3://capitalbikesharedata/transit_placemarkers/placemarks_data.csv\"\ndata_placemarks = spark.read.csv(s3_path_placemarks, header=True, inferSchema=True)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Log the actual schema\nprint(\"Actual schema for placemarks_data.csv:\")\nprint(data_placemarks.schema)\n\n# Function to validate schema field by field\ndef validate_schema(expected_schema, actual_schema):\n    failed_fields = []\n    \n    for expected, actual in zip(expected_schema, actual_schema):\n        if expected.name != actual.name:\n            failed_fields.append(f\"Field name mismatch: Expected '{expected.name}', Found '{actual.name}'\")\n        elif type(expected.dataType) != type(actual.dataType):\n            failed_fields.append(f\"Field type mismatch: Field '{expected.name}' - Expected '{expected.dataType}', Found '{actual.dataType}'\")\n    \n    if failed_fields:\n        print(\"Schema validation failed for placemarks_data.csv\")\n        for error in failed_fields:\n            print(error)\n    else:\n        print(\"Schema validation successful for placemarks_data.csv\")\n\n# Validate schema\nvalidate_schema(expected_schema_placemarks, data_placemarks.schema)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "Actual schema for placemarks_data.csv:\nStructType([StructField('id', StringType(), True), StructField('networkName', StringType(), True), StructField('networkId', IntegerType(), True), StructField('color', StringType(), True), StructField('textColor', StringType(), True), StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('type', StringType(), True), StructField('location', StringType(), True)])\nSchema validation successful for placemarks_data.csv\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}